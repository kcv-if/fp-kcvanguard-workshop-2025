{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11193215,"sourceType":"datasetVersion","datasetId":6987767},{"sourceId":328909,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":275969,"modelId":296859},{"sourceId":337122,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":282059,"modelId":302930}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"source:\n- [link](https://medium.com/codex/building-a-vanilla-gan-with-pytorch-ffdf26275b70)\n- [link](https://www.kaggle.com/code/rafat97/pytorch-vanilla-gan)","metadata":{"id":"4X4xgO2qJ9cA"}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nfrom torch import nn, optim\n!pip install torchsummary\nfrom torchsummary import summary\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom typing import Any, Callable, Optional\nfrom tqdm import tqdm","metadata":{"id":"F5GaIzTl2xoK","outputId":"25a17edf-ee6b-4d24-953e-2e85a2d01bf9","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:12.309293Z","iopub.execute_input":"2025-04-14T07:56:12.309751Z","iopub.status.idle":"2025-04-14T07:56:15.702021Z","shell.execute_reply.started":"2025-04-14T07:56:12.309692Z","shell.execute_reply":"2025-04-14T07:56:15.701176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nDATASET_PATH = '/kaggle/input/batik-dataset-for-gan/Dataset Final'\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nSHUFFLE = True\nPIN_MEMORY = False\n\nRESOLUTION = 128\nLATENT_DIM = 512\n\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 200","metadata":{"id":"ccJ0RwA83WXT","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:15.703450Z","iopub.execute_input":"2025-04-14T07:56:15.703686Z","iopub.status.idle":"2025-04-14T07:56:15.708576Z","shell.execute_reply.started":"2025-04-14T07:56:15.703665Z","shell.execute_reply":"2025-04-14T07:56:15.707557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BatikGANDataset(Dataset):\n    '''\n    BatikGAN Dataset Implementation with lazy loading.\n\n    Args:\n        path (str): Path to image directory.\n        transform (callable, optional): Image transforms that takes a PIL.Image as input. Default value is None.\n    '''\n\n    def __init__(\n        self,\n        path: str,\n        transform: Optional[Callable[Image.Image, Any]] = None\n    ) -> None:\n        super(BatikGANDataset, self).__init__()\n        self.path = path\n        self.transform = transform\n        self.files = [ f for f in os.listdir(self.path) if f.endswith(('.png', '.jpg', '.jpeg')) ]\n\n    def __len__(self) -> int:\n        return len(self.files)\n\n    def __getitem__(self, index: int) -> torch.Tensor:\n        img_path = os.path.join(self.path, self.files[index])\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img","metadata":{"id":"X7Bpj92nWCjH","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:15.710098Z","iopub.execute_input":"2025-04-14T07:56:15.710294Z","iopub.status.idle":"2025-04-14T07:56:15.727033Z","shell.execute_reply.started":"2025-04-14T07:56:15.710277Z","shell.execute_reply":"2025-04-14T07:56:15.726290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, resolution, latent_dim, hidden_dim=512, channels=3):\n        super(Generator, self).__init__()\n        output_dim = resolution * resolution * channels\n\n        self.layers = nn.Sequential(\n            self.gen_block(latent_dim, hidden_dim),\n            self.gen_block(hidden_dim, hidden_dim*2),\n             self.gen_block(hidden_dim*2, hidden_dim*2),\n            self.gen_block(hidden_dim*2, hidden_dim),\n            self.gen_block(hidden_dim, hidden_dim),\n            self.gen_block(hidden_dim, hidden_dim//2),\n            \n            nn.Linear(hidden_dim//2, output_dim),\n            nn.Tanh()\n        )\n\n    def gen_block(self, input_dim, output_dim):\n        return nn.Sequential(\n            nn.Linear(input_dim, output_dim, bias=False),\n            nn.BatchNorm1d(output_dim, 0.8),\n            nn.LeakyReLU(0.2)\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"D44OizrV53lb","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:15.728255Z","iopub.execute_input":"2025-04-14T07:56:15.728515Z","iopub.status.idle":"2025-04-14T07:56:15.744482Z","shell.execute_reply.started":"2025-04-14T07:56:15.728486Z","shell.execute_reply":"2025-04-14T07:56:15.743781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen = Generator(RESOLUTION, LATENT_DIM, channels=1).to(DEVICE)\nsummary(gen, input_size=(LATENT_DIM,), batch_size=BATCH_SIZE)","metadata":{"id":"IG7NRiME_MOK","outputId":"4df51f73-82d0-4afa-d8a1-5fe4fe96043e","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:15.745086Z","iopub.execute_input":"2025-04-14T07:56:15.745275Z","iopub.status.idle":"2025-04-14T07:56:16.260503Z","shell.execute_reply.started":"2025-04-14T07:56:15.745254Z","shell.execute_reply":"2025-04-14T07:56:16.259577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, resolution, hidden_dim=512, channels=3):\n        super(Discriminator, self).__init__()\n        input_dim = resolution * resolution * channels\n        \n        self.layers = nn.Sequential(\n            self.disc_block(input_dim, hidden_dim*4, dropout=0.3),\n            self.disc_block(hidden_dim*4, hidden_dim*2, dropout=0.3),\n            self.disc_block(hidden_dim*2, hidden_dim, dropout=0.3),\n            self.disc_block(hidden_dim, hidden_dim//2, dropout=0),  \n            nn.Linear(hidden_dim//2, 1),\n        )\n        \n    def disc_block(self, input_dim, output_dim, dropout=0):\n        layers = [\n            nn.Linear(input_dim, output_dim),\n            nn.LeakyReLU(0.2, inplace=True)\n        ]\n        if dropout > 0:\n            layers.append(nn.Dropout(dropout))\n        return nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"4ZR0c1EIHxQ_","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.261294Z","iopub.execute_input":"2025-04-14T07:56:16.261498Z","iopub.status.idle":"2025-04-14T07:56:16.267587Z","shell.execute_reply.started":"2025-04-14T07:56:16.261480Z","shell.execute_reply":"2025-04-14T07:56:16.266780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"disc = Discriminator(RESOLUTION, channels=1).to(DEVICE)\nsummary(disc, input_size=(RESOLUTION*RESOLUTION*1,), batch_size=BATCH_SIZE)","metadata":{"id":"3Q4uaBChIttE","outputId":"ed663d01-4af2-46cd-812e-5294f227b7c0","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.268344Z","iopub.execute_input":"2025-04-14T07:56:16.268634Z","iopub.status.idle":"2025-04-14T07:56:16.692675Z","shell.execute_reply.started":"2025-04-14T07:56:16.268603Z","shell.execute_reply":"2025-04-14T07:56:16.691801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def noise(batch_size, latent_dim, device):\n    return torch.randn(batch_size, latent_dim, device=DEVICE)","metadata":{"id":"0qG7F1CTGDfv","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.693550Z","iopub.execute_input":"2025-04-14T07:56:16.693911Z","iopub.status.idle":"2025-04-14T07:56:16.697793Z","shell.execute_reply.started":"2025-04-14T07:56:16.693877Z","shell.execute_reply":"2025-04-14T07:56:16.696962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_loader(resolution):\n    transform = transforms.Compose([\n        transforms.Resize((resolution, resolution)),\n        transforms.Grayscale(num_output_channels=1),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5], inplace=True)\n    ])\n\n    dataset = BatikGANDataset(DATASET_PATH, transform=transform)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\n    return loader","metadata":{"id":"6HB-QqCnUckq","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.700117Z","iopub.execute_input":"2025-04-14T07:56:16.700321Z","iopub.status.idle":"2025-04-14T07:56:16.714417Z","shell.execute_reply.started":"2025-04-14T07:56:16.700303Z","shell.execute_reply":"2025-04-14T07:56:16.713587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\n\noptim_g = optim.Adam(gen.parameters(), lr=LEARNING_RATE)\noptim_d = optim.Adam(disc.parameters(), lr=LEARNING_RATE)","metadata":{"id":"UCAXJp8oXF5F","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.715936Z","iopub.execute_input":"2025-04-14T07:56:16.716226Z","iopub.status.idle":"2025-04-14T07:56:16.730394Z","shell.execute_reply.started":"2025-04-14T07:56:16.716194Z","shell.execute_reply":"2025-04-14T07:56:16.729748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(generator, discriminator, optim_g, optim_d, epoch, folder=\"checkpoints\"):\n    os.makedirs(folder, exist_ok=True)\n\n    checkpoint = {\n        'generator': generator.state_dict(),\n        'discriminator': discriminator.state_dict(),\n        'optim_g': optim_g.state_dict(),\n        'optim_d': optim_d.state_dict(),\n        'epoch': epoch,\n    }\n\n    filename = f\"{folder}/checkpoint_epoch{epoch}.pt\"\n    torch.save(checkpoint, filename)\n    print(f\"Saved checkpoint: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.731317Z","iopub.execute_input":"2025-04-14T07:56:16.731595Z","iopub.status.idle":"2025-04-14T07:56:16.745565Z","shell.execute_reply.started":"2025-04-14T07:56:16.731566Z","shell.execute_reply":"2025-04-14T07:56:16.744945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_result(generator):\n    torch.save(generator.state_dict(), \"generator_final.pt\")\n    print(\"Final generator model saved as generator_final.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.746339Z","iopub.execute_input":"2025-04-14T07:56:16.746611Z","iopub.status.idle":"2025-04-14T07:56:16.764218Z","shell.execute_reply.started":"2025-04-14T07:56:16.746582Z","shell.execute_reply":"2025-04-14T07:56:16.763535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G_losses = []\nD_losses = []\ndef train(channels=3):\n    fixed_noise = torch.randn(16, LATENT_DIM, device=DEVICE)\n    dataloader = get_loader(RESOLUTION)\n\n    gen.train()\n    disc.train()\n\n    print(\"Starting Training...\\n\")\n\n    for epoch in range(NUM_EPOCHS):\n        gen_loss_epoch = 0.0\n        disc_loss_epoch = 0.0\n        steps = 0\n        start = time.time()\n\n        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        for batch_idx, real in pbar:\n            real = real.to(DEVICE)\n            batch_size = real.size(0)\n            real = real.view(batch_size, -1)\n\n            ### Train Discriminator ###\n            optim_d.zero_grad()\n\n            real_labels = torch.ones((batch_size, 1), device=DEVICE)\n            real_preds = disc(real)\n            real_loss = criterion(real_preds, real_labels)\n\n            noise = torch.randn(batch_size, LATENT_DIM, device=DEVICE)\n            fake = gen(noise)\n            fake_labels = torch.zeros((batch_size, 1), device=DEVICE)\n            fake_preds = disc(fake.detach())\n            fake_loss = criterion(fake_preds, fake_labels)\n\n            disc_loss = (real_loss + fake_loss) / 2\n            disc_loss.backward()\n            optim_d.step()\n\n            ### Train Generator ###\n            optim_g.zero_grad()\n\n            gen_preds = disc(fake)\n            gen_labels = torch.ones((batch_size, 1), device=DEVICE)\n            gen_loss = criterion(gen_preds, gen_labels)\n\n            gen_loss.backward()\n            optim_g.step()\n\n            # Logging\n            gen_loss_epoch += gen_loss.item()\n            disc_loss_epoch += disc_loss.item()\n            steps += 1\n\n            pbar.set_postfix({\n                \"D_loss\": f\"{disc_loss.item():.4f}\",\n                \"G_loss\": f\"{gen_loss.item():.4f}\"\n            })\n\n        print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Time: {time.time()-start:.2f}s \"\n              f\"| G Loss: {gen_loss_epoch/steps:.4f} | D Loss: {disc_loss_epoch/steps:.4f}\")\n        \n        G_losses.append(gen_loss_epoch / steps)\n        D_losses.append(disc_loss_epoch / steps)\n\n        # Save model and sample generator images every 20 epochs\n        if (epoch+1) % 20 == 0:\n            with torch.no_grad():\n                samples = gen(fixed_noise).reshape(-1, channels, RESOLUTION, RESOLUTION)\n                samples = (samples + 1) / 2\n                os.makedirs(\"generated_images\", exist_ok=True)\n                save_image(samples, f\"generated_images/epoch_{epoch+1}.png\", nrow=4)\n                print(f\"Saved generated image: epoch_{epoch+1}.png\")\n                \n            save_checkpoint(gen, disc, optim_g, optim_d, epoch + 1)\n        if epoch == (NUM_EPOCHS-1):\n            save_result (gen)","metadata":{"id":"nQxBFfdpZecm","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.764859Z","iopub.execute_input":"2025-04-14T07:56:16.765061Z","iopub.status.idle":"2025-04-14T07:56:16.775350Z","shell.execute_reply.started":"2025-04-14T07:56:16.765042Z","shell.execute_reply":"2025-04-14T07:56:16.774649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train(channels=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.775996Z","iopub.execute_input":"2025-04-14T07:56:16.776236Z","iopub.status.idle":"2025-04-14T07:56:16.792138Z","shell.execute_reply.started":"2025-04-14T07:56:16.776204Z","shell.execute_reply":"2025-04-14T07:56:16.791519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(G_losses, label=\"Generator Loss\")\nplt.plot(D_losses, label=\"Discriminator Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"loss_plot.png\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:16.792885Z","iopub.execute_input":"2025-04-14T07:56:16.793114Z","iopub.status.idle":"2025-04-14T07:56:17.191606Z","shell.execute_reply.started":"2025-04-14T07:56:16.793094Z","shell.execute_reply":"2025-04-14T07:56:17.190778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install -q onnx torchinfo torchmetrics[image]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:17.192662Z","iopub.execute_input":"2025-04-14T07:56:17.193015Z","iopub.status.idle":"2025-04-14T07:56:20.841302Z","shell.execute_reply.started":"2025-04-14T07:56:17.192981Z","shell.execute_reply":"2025-04-14T07:56:20.840280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.image.perceptual_path_length import PerceptualPathLength\nfrom torchvision.utils import save_image\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:20.842377Z","iopub.execute_input":"2025-04-14T07:56:20.842724Z","iopub.status.idle":"2025-04-14T07:56:24.368089Z","shell.execute_reply.started":"2025-04-14T07:56:20.842675Z","shell.execute_reply":"2025-04-14T07:56:24.367352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"REAL_DIR = 'fid_images/real'\nFAKE_DIR = 'fid_images/fake'\nPT_PATH = '/kaggle/input/vanillagan-bnw/pytorch/default/1/generator_final.pt'\n\ngenerator = Generator(RESOLUTION, LATENT_DIM, channels=1).to(DEVICE)\ngenerator.load_state_dict(torch.load(PT_PATH, map_location=DEVICE))\n\nos.makedirs(REAL_DIR, exist_ok=True)\nos.makedirs(FAKE_DIR, exist_ok=True)\n\ntest_loader = get_loader(RESOLUTION)\n\ncounter = 0\nfor real_img in tqdm(test_loader, desc='Saving images'):\n    real_img = real_img.to(DEVICE)\n    b = real_img.size(0)\n\n    z = torch.randn(b, LATENT_DIM, device=DEVICE)\n    fake_img = generator(z)\n    fake_img = fake_img.view(b, 1, RESOLUTION, RESOLUTION)\n\n    # Denormalize dari [-1, 1] ke [0, 1]\n    real_img = (real_img * 0.5 + 0.5).clamp(0, 1)\n    fake_img = (fake_img * 0.5 + 0.5).clamp(0, 1)\n\n    for i in range(b):\n        save_image(real_img[i], f\"{REAL_DIR}/real_{counter + i}.png\")\n        save_image(fake_img[i], f\"{FAKE_DIR}/fake_{counter + i}.png\")\n\n    counter += b\n    del z, real_img, fake_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:56:24.368863Z","iopub.execute_input":"2025-04-14T07:56:24.369213Z","iopub.status.idle":"2025-04-14T07:57:32.183271Z","shell.execute_reply.started":"2025-04-14T07:56:24.369192Z","shell.execute_reply":"2025-04-14T07:57:32.182261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\nimport os\n\nfid = FrechetInceptionDistance(feature=2048).to(DEVICE)\n\ntransform = transforms.Compose([\n    transforms.Resize((RESOLUTION, RESOLUTION)),\n    transforms.ToTensor(),\n])\n\n# Load real images\nfor file in os.listdir(REAL_DIR):\n    img_path = os.path.join(REAL_DIR, file)\n    img = Image.open(img_path).convert(\"L\") \n    img = img.convert(\"RGB\")\n    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n    img_uint8 = (img_tensor * 255).clamp(0, 255).to(torch.uint8)\n    fid.update(img_uint8, real=True)\n\n# Load fake images\nfor file in os.listdir(FAKE_DIR):\n    img = Image.open(os.path.join(FAKE_DIR, file)).convert(\"RGB\")\n    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n    img_uint8 = (img_tensor * 255).clamp(0, 255).to(torch.uint8)\n    fid.update(img_uint8, real=False)\n\n# Compute FID\nfid_score = fid.compute().item()\nprint(f'FID from folders: {fid_score:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:57:32.184561Z","iopub.execute_input":"2025-04-14T07:57:32.184941Z","iopub.status.idle":"2025-04-14T07:57:32.188923Z","shell.execute_reply.started":"2025-04-14T07:57:32.184904Z","shell.execute_reply":"2025-04-14T07:57:32.187928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image.inception import InceptionScore\n\nis_metric = InceptionScore().to(DEVICE)\ntransform = transforms.Compose([\n    transforms.Resize((RESOLUTION, RESOLUTION)),\n    transforms.ToTensor(),\n])\n\nfor file in os.listdir(FAKE_DIR):\n    img = Image.open(os.path.join(FAKE_DIR, file)).convert(\"RGB\")\n    img_tensor = transform(img).unsqueeze(0).to(DEVICE)  # [1, 3, H, W]\n    img_uint8 = (img_tensor * 255).clamp(0, 255).to(torch.uint8)\n    is_metric.update(img_uint8)\n\nis_score = is_metric.compute()\nprint(f\"Inception Score: {is_score[0].item():.4f} Â± {is_score[1].item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:59:02.748146Z","iopub.status.idle":"2025-04-14T07:59:02.748528Z","shell.execute_reply":"2025-04-14T07:59:02.748365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image.perceptual_path_length import PerceptualPathLength\nclass PPLWrapper(nn.Module):\n    def __init__(self):\n        super(PPLWrapper, self).__init__()\n\n    def forward(self, z: torch.Tensor) -> torch.Tensor:\n        return generator(z)\n\n    def sample(self, num_samples: int) -> torch.Tensor:\n        return torch.randn(num_samples, LATENT_DIM, device=DEVICE)\n\ngenerator.eval()\nppl = PerceptualPathLength().to(DEVICE)\n\nppl_mean, ppl_std, ppl_raw = ppl(PPLWrapper())\nprint(f'PPL: {ppl_mean.item()} +/- {ppl_std.item()}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:58:44.171831Z","iopub.execute_input":"2025-04-14T07:58:44.172098Z","iopub.status.idle":"2025-04-14T07:59:02.583783Z","shell.execute_reply.started":"2025-04-14T07:58:44.172076Z","shell.execute_reply":"2025-04-14T07:59:02.582874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator.eval()\n\n# Generate 64 gambar\nz = torch.randn(64, LATENT_DIM, device=DEVICE)\nwith torch.no_grad():\n    fake_img = generator(z)\n    fake_img = fake_img.view(64, 1, RESOLUTION, RESOLUTION)\n    fake_img = (fake_img * 0.5 + 0.5).clamp(0, 1)  # denormalize dari [-1, 1] ke [0, 1]\n\n# Simpan dalam satu grid 8x8\nsave_image(fake_img, \"batik_grid.png\", nrow=8)\nprint(\"Saved batik grid image as 'batik_grid.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:59:02.745662Z","iopub.status.idle":"2025-04-14T07:59:02.745961Z","shell.execute_reply":"2025-04-14T07:59:02.745854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dummy_input = torch.randn(1, LATENT_DIM, device=DEVICE)\n\ntorch.onnx.export(\n    generator,                         \n    dummy_input,                      \n    \"generator.onnx\",                 \n    input_names=[\"latent_vector\"],     \n    output_names=[\"generated_image\"],  \n    dynamic_axes={\n        \"latent_vector\": {0: \"batch_size\"},\n        \"generated_image\": {0: \"batch_size\"},\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:59:15.738172Z","iopub.execute_input":"2025-04-14T07:59:15.738457Z","iopub.status.idle":"2025-04-14T07:59:16.750651Z","shell.execute_reply.started":"2025-04-14T07:59:15.738438Z","shell.execute_reply":"2025-04-14T07:59:16.749669Z"}},"outputs":[],"execution_count":null}]}